# Knowledge Distillation

A general explanation of how knowledge distillation can aid in training NLP/CV-based tasks by refining logits for more useful outcomes.

The following ideas aim to efficiently extract the right information from NLP models, such as LLMs.
